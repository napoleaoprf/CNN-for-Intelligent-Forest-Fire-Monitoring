{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a75e1-f290-4b2b-b691-1cc2b07a32f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization, Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import sklearn\n",
    "import skimage\n",
    "from skimage.transform import resize\n",
    "import random\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de566f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diretório com o dataset utilizado para treinamento\n",
    "train = \"E:\\Dados\\Training and Validation\"\n",
    "\n",
    "#Diretório com o dataset utilizado para testes\n",
    "test = \"E:\\Dados\\Testing\"\n",
    "\n",
    "\n",
    "#Define que os dados devem ser carregados a partir de imagens originais\n",
    "LOAD_FROM_IMAGES = True\n",
    "\n",
    "# Lendo as imagens de um diretório e convertendo em arrays numéricos (x) \n",
    "# junto com as respectivas classes (y).\n",
    "def get_data(folder):\n",
    "    x = []\n",
    "    y = []\n",
    "    #Percorrendo o diretório 'folder'\n",
    "    for folderName in os.listdir(folder):\n",
    "        if not folderName.startswith(\".\"):\n",
    "            #Com base no nome do diretório,será definida a classe\n",
    "            if folderName in [\"nofire\"]:\n",
    "                label = 0\n",
    "            elif folderName in [\"fire\"]:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 2\n",
    "            #Percorrendo as imagens dentro de cada classe\n",
    "            for image_filename in tqdm(os.listdir(folder +\"/\" +folderName+\"/\")):\n",
    "                new_size = (90, 90)\n",
    "                img_arr = cv2.imread(folder + \"/\" + folderName + \"/\" + image_filename)\n",
    "                if img_arr is not None:\n",
    "                    #cada nova imagem é carregada e redimensionada para 128x128 pixels\n",
    "                    img_arr = cv2.resize(img_arr, new_size)\n",
    "                    # os arrays de imagem e classe são adicionados às listas \n",
    "                    x.append(img_arr)\n",
    "                    y.append(label)\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    return x,y\n",
    "\n",
    "if LOAD_FROM_IMAGES:\n",
    "    X_train,y_train = get_data(train)\n",
    "    X_test, y_test = get_data(test)\n",
    "    \n",
    "    np.save(\"xtrain.npy\",X_train)\n",
    "    np.save(\"ytrain.npy\",y_train)\n",
    "    np.save(\"xtest.npy\",X_test)\n",
    "    np.save(\"ytest.npy\",y_test)\n",
    "else:\n",
    "    X_train = np.load(\"xtrain.npy\")\n",
    "    y_train = np.load(\"ytrain.npy\")\n",
    "    X_test = np.load(\"xtest.npy\")\n",
    "    y_test = np.load(\"ytest.npy\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# dividindo os dados de treinamento em dados de treinamento e validação. \n",
    "# Aqui está  divide em uma proporção de 80% para treinamento e 20% para validação.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train,test_size=0.2,shuffle=True)\n",
    "\n",
    "# Normaliza os valores dos pixels das imagens dividindo-os por 255.0 \n",
    "# para trazer todos os valores entre 0 e 1\n",
    "X_train, X_valid, X_test = X_train / 255.0, X_valid / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9e74b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81126eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# camada de convolução com 32 filtros, cada um com uma janela de convolução de 11x11. \n",
    "# Os filtros são aplicados com um passo de 4 pixels horizontalmente e 4 pixels verticalmente.\n",
    "model.add(layers.Conv2D(32,(11,11),strides=(4, 4), input_shape=(90,90,3), use_bias=False))\n",
    "# Adiciona uma camada de normalização em lotes após a camada de convolução para normalizar\n",
    "# os valores de ativação da camada anterior\n",
    "model.add(BatchNormalization())\n",
    "# Aplica a função de ativação ReLU (Rectified Linear Unit) para introduzir não-linearidade.\n",
    "model.add(layers.Activation('relu'))\n",
    "# Adiciona uma camada de max pooling para reduzir a dimensão espacial dos mapas de \n",
    "# características resultantes. Usa uma janela de pooling de 3x3 e \n",
    "# um passo de 2 pixels horizontalmente e 2 pixels verticalmente\n",
    "model.add(layers.MaxPooling2D((3,3), strides=(2,2)))\n",
    "# Adiciona outra camada de convolução com 32 filtros, cada um com uma janela de \n",
    "# convolução de 5x5. A opção padding=\"same\" mantém a saída da camada com a mesma \n",
    "# dimensão espacial da entrada.\n",
    "model.add(layers.Conv2D(32,(5,5),padding=\"same\", use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D((3,3), strides=(2,2)))\n",
    "model.add(layers.MaxPooling2D((3,3), strides=(2,2)))\n",
    "\n",
    "# Transforma as saídas 2D das camadas anteriores em um vetor 1D, \n",
    "# preparando os dados para as camadas totalmente conectadas (Dense).\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Adiciona uma camada densamente conectada com 32 neurônios e função de ativação ReLU.\n",
    "model.add(layers.Dense(128,activation=\"relu\", use_bias=False))\n",
    "# Adiciona uma camada de dropout com taxa de 50%, o que ajuda a evitar overfitting \n",
    "# ao desativar aleatoriamente metade das unidades de neurônios durante o treinamento.\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(64,activation=\"relu\", use_bias=False))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(1,activation='sigmoid', use_bias=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a3a1b4-abad-45fb-8a5a-1ecc279ce63b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2042cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\" , loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"accuracy\",\n",
    "    patience=50,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "batch_size=16\n",
    "epochs=300\n",
    "\n",
    "#history =  model.fit(X_train,y_train,validation_data=(X_valid,y_valid),batch_size=batch_size, epochs=epochs,verbose=1,callbacks=[early_stopping])\n",
    "history = model.fit(datagen.flow(X_train, y_train),validation_data=(X_valid, y_valid),batch_size=batch_size,epochs=epochs, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98013e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('Test loss:', round(score[0],4))\n",
    "print('Test accuracy:', round(score[1],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12c495a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = (y_test_pred > 0.5)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred.shape\n",
    "\n",
    "y_test.shape\n",
    "\n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bd34e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fire_forest_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc989a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
